

## 概念

```python
分类器 : 分类方法的统称
    
评估器 : 用于训练的算法

超参数 : 训练前设置的参数,而不是训练得到的参数

```



## 基本方法

```python
.fit()      # 计算
.transform()
.fit_transform()


```





# 1.预处理部分



# 2.特征工程部分



```python
# 归一化
  消除数据量纲的影响,使指标间具有可比性
  (文本数据的全大写,全小写等)
# 离散化
  连续值->离散值
# 缺失值
   
# Dummy Coding

# 数据变化: log,指数,Box-Cox

```



## 特征构造

```
从 原始特 中构造 新属性  (人工行为)
```



## 特征选择

```
从 所有特征 中选择 用于训练的特征
```

### 过滤法

### 包装法

### 集成法





## 特征提取

```
将 特征矩阵 转化为 新的特征矩阵
目的: 降低维度,提取主要信息

```



### 主成分分析PCA

```python
特征工程三种方式：特征提取，特征创造和特征选择。

特征提取: 提取出新特征
特征选择: 从已存在的特征中选取携带信息最多的,选完之后的特征依然具有可解释性
特征创造:
    
PCA降维: 将已存在的特征进行压缩，建立了新的特征向量,也无法知道其含义,属于特征创造

PCA 不适用于探索特征和标签之间的关系的模型(如线性回归)，因为无法解释的 新特征 和 标签 之间的关系不具有意义。

```



```python
使用样本方差(可解释性方差)来衡量特征所带的信息量:
    对每一个特征的全部数据求方差,方差越小,越难分 
```



```python
class sklearn.decomposition.PCA (n_components=None,        # 降维后的维度
                                 ##########################################
                                 copy=True, 
                                 whiten=False, 
                                 svd_solver=’auto’, 
                                 tol=0.0,
                                 iterated_power=’auto’, 
                                 random_state=None
                                )
如何确定降维后的维度:
    
```





### 线性判别分析LDA

```

```











# 3.训练部分



## -----聚类------

```python

```

### K-means

```python
class sklearn.cluster.KMeans (
                              # 1.训练算法选择
                              algorithm='auto',
                              
                              # 2.初始化参数
                              n_clusters=8,                   # K,几个簇心
    					    init='k-means++',               # 簇心初始化
                              random_state=None,
                              n_init=10, 
    
                              # 3.计算参数
                              precompute_distances='auto',
                              verbose=0, 
    
                             # 4.训练提前停止
                              max_iter=300,                   # 最大迭代次数(让计算提前停下)
                              tol=0.0001,                     # 两次迭代的最小下降量
                              
                             # 5.训练速度(硬件相关)
                              n_jobs=None, 
    
    	                    # 6.其他
                              copy_x=True, 
                             )

```



```python
# 属性
.cluster_centers_   # 簇心
.labels_            # 每个样本点对应标签
.inertia_           # 簇内平方和(每个样本到簇心的均方距离)
.n_iter_            # 实际迭代次数
```



```python
# 方法
# 训练
.fit(X)
.
```













## -----分类------

### 树

#### 0.核心问题

```python
# 1.选择最优划分属性(如何分支)
    依据不纯度，分类树的不纯度用基尼系数或信息熵来衡量
               回归树的不纯度用MSE均方误差来衡量
    每次分枝时，决策树对所有的特征进行不纯度计算，选取不纯度最低的特征进行分枝
# 2.树生长到什么时候应该停下(如何防止过拟合)       

```





#### 1.分类树(决策树)

```
信息熵对不纯度更敏感,高维数据/有噪音 容易过拟合;  

若过拟合,考虑使用 基尼指数
若欠拟合,考虑使用 信息熵
```





```python
from sklearn import tree
tree.DecisionTreeClassifier(criterion='gini',                 # 不纯度 "gini":基尼系数  "entropy":信息熵
                            splitter='best',                  # 分支方式 "best":选择最有属性分 "random":随机分
                            
                            max_depth=None,                   # 树的高度, 适合样本少,维度高时剪枝;建议从3开始测试
                            max_features=None,                # 最多选择多少个属性参与划分 适合样本少,维度高时剪枝
                            
                            min_samples_split=2,              # 当前节点最小样本数
                            min_samples_leaf=1,               # 子节点最小样本数;(太小:过拟合;太大:欠拟合)
                            min_weight_fraction_leaf=0.0,     # 权重,调整建模方向;给少部分赋予大权重可少数类方向建模
                            
                            random_state=None,                # 随机种子
                            class_weight=None, 
                            
                            ###################################################################################
                            max_leaf_nodes=None, 
                            min_impurity_decrease=0.0,       # 最小信息增益
                            min_impurity_split=None,
                            presort='deprecated', 
                            ccp_alpha=0.0
                            )

```





```python
# 所有标签的数组
class_    
# 每个特征的重要性的数组
feature_importances_
# 最重要的特征
max_featureS_
# 训练模型使用的特征个数
n_features_
# 训练模型输出的结果个数
n_outputs_

```





```python
# 训练
.fit(X=train_X,y=train_Y,sample_weigth=)

# 预测,返回label
.predict(test_X)
# 每个样本对应各个标签的概率 (标签按词典"顺序排序")
predict_proba(test_X)
# 预测,返回对于叶节点索引
.apply(test_X)

# 评分
.score(X=实际标签,y=预测标签,sample_weight=)


```



#### 2.回归树

```python
class sklearn.tree.DecisionTreeRegressor(criterion='mse', # mse:       friedman_mse:       mae:
                                         splitter='best', 
                                         
                                         max_depth=None, 
                                         max_features=None, 
                                         
                                         min_samples_split=2, 
                                         min_samples_leaf=1, 
                                         min_weight_fraction_leaf=0.0, 
                                         
                                         random_state=None, 
                                         
                                         ##########################################################
                                         max_leaf_nodes=None, 
                                         min_impurity_decrease=0.0, 
                                         min_impurity_split=None, 
                                         presort='deprecated', 
                                         ccp_alpha=0.0
                                        )

```







### 支持向量机

```python
class sklearn.svm.SVC (C=1.0,                           # C大,边界小,准确度高
                       kernel='rbf',                    # 核函数
                       
                       
                       degree=3,                        # 多项式次数
                       gamma='auto_deprecated',         # 核函数系数
                       coef0=0.0, 
                       shrinking=True,
                       probability=False, 
                       tol=0.001, 
                       cache_size=200, 
                       class_weight=None, 
                       verbose=False, 
                       max_iter=-1,
                       decision_function_shape='ovr', 
                       random_state=None
                    )

```







### 朴素贝叶斯

```

```





#### 高斯朴素贝叶斯

#### 多项式朴素贝叶斯

#### 伯努利朴素贝叶斯

#### 补集朴素贝叶斯



















## -----回归------

### 逻辑回归



```python

```



```python
损失函数，正则化，梯度下降，海森矩阵
```







### 线性回归







### 多项式回归









## -----集成------

```python
# 集成学习: 本身不是一个单独的机器学习算法，而是通 过在数据上构建多个模型，集成所有模型的建模结果
           #集成算法 考虑多个评估器的建模结果,汇总后得到一个综合结果,来获取比单个模型更好的 回归 或 分类表现 .
# 集成评估器由 基评估器 组成

  集成方法: 
        1.袋装法(Bagging) :  模型独立,结果采用平均或多数表决 (如:随机森林)
        2.提升法(Boosting):  模型相关,结合弱评估器的力量一次次对难以评估的样本进行预测,构成一个强评估器。
                             (如:Adaboost,梯度提升树)
        3.stacking       :  

```



### 随机森林



#### 0.核心问题

```python
# 1.选择最优划分属性(如何分支)
    依据不纯度，分类树的不纯度用基尼系数或信息熵来衡量
               回归树的不纯度用MSE均方误差来衡量
    每次分枝时，决策树对所有的特征进行不纯度计算，选取不纯度最低的特征进行分枝
# 2.树生长到什么时候应该停下(如何防止过拟合)        (同树)

```



#### 1.随机森林分类

```python
class sklearn.ensemble.RandomForestClassifier(n_estimators=100,    
                                              
                                              criterion='gini',    # 不纯度 "gini":基尼系数  "entropy":信息熵
                                              min_samples_split=2, # 当前节点最小样本数
                                              min_samples_leaf=1,  # 子节点最小样本数;(太小:过拟合;太大:欠拟合)
                                              min_weight_fraction_leaf=0.0,# 权重,给少部分赋予大权重可少数类方向建模
                                              
                                              max_depth=None,       # 树的高度, 适合样本少,维度高时剪枝;建议从3开始测试
                                              max_features='auto',  # 最多选择多少个属性参与划分 适合样本少,维度高时剪枝
                                              
                                              min_impurity_decrease=0.0,  # 最小信息增益(小于此值不分枝)
                                              min_impurity_split=None,   
                                              
                                              random_state=None,
                                              class_weight=None, 
                                              
                                              ##################################################
                                              bootstrap=True, 
                                              oob_score=False, 
                                              n_jobs=None,  
                                              verbose=0, 
                                              warm_start=False,
                                              ccp_alpha=0.0, 
                                              max_samples=None
                                             )

```







#### 2.随机森林回归

```python
class sklearn.ensemble.RandomForestRegressor(n_estimators=100, 
                                             criterion='mse', 
                                             max_depth=None, 
                                             min_samples_split=2, 
                                             min_samples_leaf=1, 
                                             min_weight_fraction_leaf=0.0, 
                                             max_features='auto', 
                                             max_leaf_nodes=None, 
                                             min_impurity_decrease=0.0, 
                                             min_impurity_split=None, 
                                             bootstrap=True, 
                                             oob_score=False, 
                                             n_jobs=None, 
                                             random_state=None, 
                                             verbose=0, 
                                             warm_start=False, 
                                             ccp_alpha=0.0, 
                                             max_samples=None
                                            )

```







### XGBoost

```python
# 极致梯度提升树
树模型
CART : 分类回归树
GBDT : 梯度提升决策树
XGB  : 极致梯度提升(基于决策树)

  
  
```

```python
class xgboost.DMatrix(data, 
                      label=None, 
                      weight=None, 
                      base_margin=None, 
                      missing=None, 
                      silent=False, 
                      feature_names=None, 
                      feature_types=None, 
                      nthread=None              # 线程数
                     )


params {eta,                     # 学习率
        gamma, 
        max_depth, 
        min_child_weight,
        max_delta_step, 
        subsample, 
        colsample_bytree,
        colsample_bylevel, 
        colsample_bynode,
        lambda, 
        alpha, 
        tree_method string,
        sketch_eps, 
        scale_pos_weight, 
        updater,refresh_leaf, 
        process_type, 
        grow_policy, 
        max_leaves,
        max_bin, 
        predictor, 
        num_parallel_tree
       }


xgboost.train (params, 
               dtrain, 
               num_boost_round=10, 
               evals=(),
               obj=None, 
               feval=None, 
               maximize=False,
               early_stopping_rounds=None, 
               evals_result=None, 
               verbose_eval=True, 
               xgb_model=None, 
               callbacks=None,
               learning_rates=None
              )

```





### stacking



### Blending









# 4.模型评估部分

```python
 损失函数
	求一个样本 预测Y 与 实际Y 不一致的情况
1.风险项
    测试集全部样本 损失的平均
2.正则项
    通过影响权重来解决过拟合问题
3.风险结构成 = 风险项 + 正则项

# 概率: 研究 自变量   和 因变量 之间的关系

# 似然: 研究 参数取值 和 因变量 之间的关系

```



![1](https://cdn.jsdelivr.net/gh/north151/ImageBed/image_Windows/1.png)

```python
# 梯度下降法 => 最小化损失函数
  f(X)=XW+b  
    1.Loss(w,b) = 1/N * (sum[1,m](不一致程度,yi和f(wxi+b)的函数) )
    
    2.在 X=test_X,W=上一次训练出来的W,b=上一次训练出来的b 情况下 
      对 LOSS(w,b) 求偏导得 dL/dw  dL/db  # (矩阵求导,不用管它)
      更新 w -= 学习率*dL/dw   b -= 学习率  (初始W,b可通过训练得到)
      (自变量是w,b;量是L; 要使L最小 dL/dw -> 0; 当dL/dw>0,L在递增,W减小L更小; 当dL/dw<0,L递减,W越大L越小)

# 梯度提升法 => 
  赋予参数初始值,从初始值开始按指定步长增加,训练找到最合适的参数
    
```





## -----聚类------









## -----分类------





## -----回归------





## -----集成------













































